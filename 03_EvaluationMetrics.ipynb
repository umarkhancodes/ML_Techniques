{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d2e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we talk about classification problems, the most common metrics used are:\n",
    "#- Accuracy\n",
    "#- Precision (P)\n",
    "#- Recall (R)\n",
    "#- F1 score (F1)\n",
    "#- Area under the ROC (Receiver Operating Characteristic) curve or simply AUC (AUC)\n",
    "#- Log loss\n",
    "#- Precision at k (P@k)\n",
    "#- Average precision at k (AP@k)\n",
    "#- Mean average precision at k (MAP@k)\n",
    "\n",
    "\n",
    "#When it comes to regression, the most commonly used evaluation metrics are:\n",
    "#- Mean absolute error (MAE)\n",
    "#- Mean squared error (MSE)\n",
    "#- Root mean squared error (RMSE)\n",
    "#- Root mean squared logarithmic error (RMSLE)\n",
    "#- Mean percentage error (MPE)\n",
    "#- Mean absolute percentage error (MAPE)\n",
    "#- R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d16dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for balanced dataset - binary classification problem \n",
    "    # we use accuracy, precision, recall, f1 \n",
    "\n",
    "# tp, tn, fp (type I error), fn (Type II error)\n",
    "\n",
    "# accuracy : describing how accurate the model is (not good for skewed datasets)\n",
    "    # (tp+tn)/(tp+tn+fp+fn)\n",
    "\n",
    "# precision : tp/(tp+fp)\n",
    "# recall : tp/(tp+fn) - same as tpr (true positive rate / senstivity)\n",
    "# false positive rate (fpr) = fp/(tn+fp) ; 1-fpr = true negative rate (specificity)\n",
    "\n",
    "# MOST IMPORTANT ARE TPR AND FPR \n",
    "\n",
    "\n",
    "# both of above ranges from 0 to 1 and closer to 1 is better \n",
    "\n",
    "# how we select the probability threshold greatly impact the value of precision and recall \n",
    " # precision - recall curve : curve of precision and recall for different values of threshold \n",
    "    \n",
    "# f1 score = 2pr/(p+r) , ranging from 0 to 1 (use this for skewed datasets)\n",
    "\n",
    "# ROC Curve : TPR (on y-axis), FPR (on x-axis) {for different value of threshold}\n",
    "# - area under this curve is Area under the ROC curve \n",
    "# AUC (area under curve) = 1 : perfect model\n",
    "# AUC = 0 : very bad \n",
    "# AUC = 0.5 : totally random\n",
    "\n",
    "# an AUC of 0.85 means the if we select a positive and a negative sample from dataset\n",
    "# the model will rank the postive sample higher then the negative sample with a probability of 0.85 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0069b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds are used to make predictions \n",
    "# value of threshold can be selected from ROC Curve \n",
    "# we want to minimize false positives, and maintain the true positives\n",
    "\n",
    "# log loss : -ylog(y^) + (1-y)log(1-y^)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f259e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi class classification problem \n",
    "\n",
    "\n",
    "# following are all one vs all approaches \n",
    "\n",
    "# macro-averaged  : calculate precision for all classes and average them\n",
    "# micro-averaged  : calculate class wise tp,etc then sum and then calculate overall precision\n",
    "# weighted  : same as macro, with weighted average (wrt number of samples )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31fec6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix  (prediction {vertical}, actual {top horizontal})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17381982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-label classification (Each sample can have one or more classes associated with it)\n",
    "# eg detect all objects present in an image \n",
    "\n",
    "# for such problems we use \n",
    "\n",
    "# precision at k {considering only top k predictions } - number of hits (intersection of true and predicted labels)/k\n",
    "# average precision at k - e.g., ap@3 = average(p@1, p@2, p@3)\n",
    "# mean average precision at k - mean (avg) of ap@k (from 1 to k)\n",
    "# log loss - mean column wise log loss (convert targets to binary and use log loss for each column)\n",
    "\n",
    "# ranging from 0 to 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6df8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ab11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error : actual_value - predicted_value\n",
    "# absolute error : abs(error)\n",
    "# mean absolute error : mean(all absolute errors)\n",
    "\n",
    "# squared error : (actual - predicted)^2\n",
    "# mean squared error : mean (all squared errors)\n",
    "# root mean squared error : sqrt ( MSE )\n",
    "\n",
    "# Squared logrithmic error  :( log(1+actual)-log(1+predicted) )^2 \n",
    "# mean squared logrithmic error : mean (of all squared logrithmic errors)\n",
    "# root mean squared logrithmic error : sqrt(MSLE)\n",
    "\n",
    "# percentage error : ((actual - predicted)/actual)*100 %\n",
    "# mean percentage error \n",
    "# mean absolute percentage error \n",
    "\n",
    "# R2 (R squared) (cooficient of determination) (0 to 1) (1 being good, 0 being bad) (can also be negative {absurd predictions})\n",
    "# 1 -  (  sum all squared errors/sum of (actual - mean of actual)^2  )\n",
    "# 1 - RSS/TSS\n",
    "# RSS = Sum of square of residuals(errors)\n",
    "# TSS = total sum of squares (actual - mean)^2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19232147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d250cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohen's kappa : used for classification (measures the aggrement between two ratings (i.e. how close they are to each other))\n",
    "# closer to 1 is better, closer to 0 is bad \n",
    "\n",
    "# MCC Mathews correlation coefficient (for classification) (-1 to 1) (-1 is bad , 0 is random , 1 is good)\n",
    "# uses tp,tn,fp,fn and is thus good for skewed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98d372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm",
   "language": "python",
   "name": "cm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
